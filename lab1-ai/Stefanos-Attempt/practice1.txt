Practice Lab 1

General Rules

If you decide to split the workload within the group, make sure to explain to each other 
what you’ve done. Each group member should be able to present the whole report.

Report structure

1.Methodology: Implementation details and experimental design
2.Results: Quantitative and qualitative findings with visualisations
3.Analysis: Answer all questions posed in the tasks

Getting help

•Use the relevant channels on the course Discord server for discussing the assignment
•When getting advice from other groups, don’t simply copy their code
•Only send direct messages to teachers regarding personal matters (no general 
questions!)


PART I: Fundamentals of Machine Learning and ANN with 
backprop algorithm

Task 1: Nearest Neighbour classifier: skills of working with datasets 
and hyperparameters

In this task you will learn fundamentals of data pre-processing and test a simplest, yet 
useful Machine Learning model as well as master your skills of working with datasets.
MNIST dataset:
One commonly used toy image classification dataset is the MNIST dataset. This dataset 
consists of 60,000 tiny images that are 28 pixels high and wide. Each image is labeled 
with one of 10 classes (“0”, “1”, “2”, “3”, “4”, “5”, “6”, “7”, “8”, “9”). These 60,000 images 
are partitioned into a training set of 50,000 images and a test set of 10,000 images.

You are provided with an implementation of a special case of the K-Nearest Neighbors 
classifier (i.e. 1-NN) using L1 norm (D7079E_code_kNN_task.ipynb). The 1-NN classifier 
takes a test image, compares it to every single one of the training images, and predicts 
the label of the closest training image.

1.The code downloads the MNIST dataset. Follow the tutorial https://hadrienj.github.io/posts/Preprocessing-for-deep-learning/
, which is also linked from Canvas and perform all the pre-processing steps 
for your dataset. You need to demonstrate the Jupyter notebook with all the steps.


2.Run the provided code for 1-NN classifier. What is the accuracy of the method?


3.Modify the code of the NearestNeighbor example so that it uses L2 norm for 
classification. How the classification accuracy has changed now?


4.The accuracy of the NN classifier that you have observed in the previous two tasks is 
VERY low. This is due to the bug in the code – find it and fix it.

HINT: Check the distances computed by Python and those computed by hand for 
several pairs of images.


5.Extend the code of the NearestNeighbor example with necessary methods, which 
implement k-NN classifier. That is instead of finding the single closest image in the 
training set, you will find the top k closest images, and have them vote on the label of 
the test image.




Task 2. Fundamentals of the Artificial Neural Networks and 
Backpropagation algorithm

In this task we will continue to classify hand-written digits from the publicly available 
dataset MNIST. This time you will use a Jupyter notebook D7079E_code-
ANN_backprop_task.ipynb, which is available in Canvas.

The goal of this task is to understand the fundamentals of the backpropagation algorithm 
and study classification performance of the multi-layer perceptron.

1.Understand the implementation structure of the multilayer perceptron in Jupyter 
notebook D7079E_code-ANN_backprop_task.ipynb.


1.Be able to explain the principle of backpropagation algorithm;
2.Be able to explain the meaning and the role of the Softmax function;
3.Be able to name typically used non-linear output functions and implications of 
choosing one or another for implementation.
4.The code in the provided Jupyter notebook will stop execution at several points. 
Find the places in the code, where the execution breaks, answer the questions, 
comment out the exit line and run the code again.


2.Run the code with the suggested configuration of the hyperparameters: number of 
epochs =70 and learning rate =0.05. What is the classification accuracy?
3.Run the code with Learning rate =0.005 and Learning rate =0.5. Explain the observed 
differences in the functionality of the multi-layer perceptron.
4.Extend the code implementing the ReLU output function. Run the perceptron with the 
suggested by default configuration of hyperparameters: number of epochs = 70 and 
learning rate =0.05. What is the classification accuracy? Find the values of the learning 
rate which results in comparable to Sigmoid case accuracy.



PART II: One-Pixel Attack

This part focuses on implementing the one-pixel attack itself and a corresponding 
defense method.
For the following tasks, look at the relevant papers for reference. You may look at existing 
implementations for inspiration but do not copy their code (wholly or partially) into your 
submission. You must write your own implementation!
Use https://docs.pytorch.org/vision/main/models/generated/torchvision.models.resnet18.html#torchvision.models.resnet18 
as the target model and https://docs.pytorch.org/vision/master/generated/torchvision.datasets.CIFAR10.html 
as the dataset.

Task 3.1: Implementing the attack

The paper for the attack is called “One pixel attack for fooling deep neural networks” by 
Vargas & Kouichi (2019).
Your implementation will need:

1.Mutation strategy
2.Fitness function
3.Constraint handling


Task 3.2: Analysis and Evaluation

Design and run experiments to answer these questions:

1.Success Rate Analysis: What percentage of images can be successfully attacked?
2.Efficiency Analysis: How many iterations does the algorithm typically need?
3.Pattern Recognition: Are certain types of images more vulnerable?


Task 4: Defending against the attack

The paper for the defense method is called “Adaptive Pixel Resilience: A Novel Defence 
Mechanism Against One-Pixel Adversarial Attacks on Deep Neural Networks” by 
Srivastava & Muskaan (2024).
Implement and test the ‘Adaptive Pixel Resilience’ defense mechanism from this paper 
and test it against your attack implementation from task 3. Your version should contain all 
3 components from paper, namely adversarial training, an additional layer with pixel-wise 
attention and regularisation.

HINT: 
You do not need to create an entirely new network for this; libraries like PyTorch 
allow you to modify the architecture of a pre-loaded model like ResNet-18.
Task 4.2: Analysis and Evaluation

Design and run experiments to answer these questions:

1.Can you reproduce the effectiveness shown in the paper?
2.What is the trade-off between security and model accuracy?
3.Could an attacker adapt their strategy to overcome this defense?


Congrats, you are now done with Lab 1!

You have just become familiar with fundamentals of Machine Learning 
and Artificial Neural Networks with back propagation training algorithm! 
You also tested the One-Pixel attack against Artificial Neural Networks.

Well done!

