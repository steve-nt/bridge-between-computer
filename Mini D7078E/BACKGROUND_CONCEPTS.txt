\section{Background \& Concepts}

\subsection{Cloud Reliability and Auto-Scaling}

Cloud reliability depends on the ability to detect failures, respond to capacity changes, and maintain service availability despite infrastructure degradation. Auto-scaling is a mechanism by which cloud infrastructure automatically adjusts the number of running instances based on demand metrics, typically CPU utilization or request count. This approach eliminates the need for manual capacity planning and enables systems to handle unexpected traffic spikes or gracefully shed load during demand reductions. Auto-scaling policies define threshold values (scale-out triggers and scale-in triggers), cooldown periods to prevent rapid scaling oscillations, and minimum/maximum capacity limits to control costs and ensure availability.

\subsection{Health Checks and Instance Replacement}

Health checks are periodic tests performed by load balancers or monitoring systems to determine whether a resource (e.g., EC2 instance) is functioning correctly. A health check typically involves sending an HTTP request to a target endpoint and verifying a successful response (e.g., HTTP 200 status). When an instance fails consecutive health checks, it is marked as unhealthy and removed from the load balancer's target group, preventing new traffic from reaching it. Auto-scaling groups monitor health status and automatically terminate unhealthy instances, triggering the launch of replacement instances to maintain desired capacity. This automated replacement mechanism is critical to achieving high availability without manual intervention.

\subsection{AWS Services for Resilient Infrastructure}

The lab utilizes key AWS services to construct a resilient, auto-scaling web infrastructure:

\begin{itemize}
    \item \textbf{EC2 (Elastic Compute Cloud):} Virtual servers that host the web application. Instances are launched from a launch template containing a pre-configured AMI.
    
    \item \textbf{Application Load Balancer (ALB):} Distributes incoming traffic across multiple EC2 instances and performs health checks to detect unhealthy targets.
    
    \item \textbf{Target Group:} A logical grouping of instances that receive traffic from the ALB. Health checks are configured at the target group level.
    
    \item \textbf{Auto Scaling Group (ASG):} Automatically launches or terminates instances based on scaling policies and health status, maintaining the desired number of healthy instances.
    
    \item \textbf{CloudWatch:} Monitors metrics (CPU utilization, request count, healthy host count) and triggers alarms when thresholds are exceeded.
    
    \item \textbf{Systems Manager (SSM):} Enables remote command execution on instances without SSH, used for controlled stress testing and failure simulation.
    
    \item \textbf{CloudTrail:} Logs all API calls and user actions for audit and compliance purposes.
\end{itemize}

These services together provide a complete infrastructure for deploying scalable, self-healing applications on AWS.

\subsection{Failure Simulation and Chaos Engineering}

Failure simulation (or chaos engineering) is the practice of deliberately introducing controlled failures into a system to test its resilience and validate that recovery mechanisms work as intended. In this lab, controlled failure is simulated by applying CPU stress to a single instance via the \texttt{stress-ng} command executed through AWS Systems Manager. This approach allows observation of:

\begin{itemize}
    \item How health checks detect degraded instances (e.g., slow HTTP responses due to high CPU load)
    
    \item How the ALB automatically removes unhealthy instances from the target group
    
    \item How the ASG automatically launches replacement instances
    
    \item Recovery timeline and the point at which service availability is restored
\end{itemize}

By simulating failures in a controlled, safe manner (limited duration, single instance, predefined targets), the lab demonstrates that the system is resilient and that monitoring/recovery mechanisms function correctly before failures occur unexpectedly in production.

\subsection{Key Metrics and Thresholds}

The lab monitors several key metrics:

\textbf{CPU Utilization:} Average CPU usage across all instances in the ASG. Scaling policies are triggered when CPU utilization reaches 80\% (scale-out) or falls below 30\% (scale-in).

\textbf{HealthyHostCount:} The number of instances currently healthy and receiving traffic from the ALB. This metric increases as instances are launched and pass health checks, and decreases when instances fail health checks or are terminated.

\textbf{UnhealthyHostCount:} The number of instances that have failed health checks and are no longer receiving traffic. This metric rises during failure simulation and falls as unhealthy instances are replaced.

\textbf{RequestCount:} The number of HTTP requests received by the ALB per unit time. This metric correlates with load generator intensity and scales with the number of healthy instances.

By monitoring these metrics, students observe the relationship between load, scaling actions, health status, and system recovery in real time.

\subsection{Infrastructure as Code Principles}

While this lab focuses primarily on cloud reliability rather than IaC, the infrastructure is defined and deployed using AWS CloudFormation templates and AWS CLI commands, which embody IaC principles:

\textbf{Declarative Definition:} Infrastructure components are defined in configuration templates rather than created manually through the AWS Console.

\textbf{Version Control:} Configuration definitions can be stored in source control systems, enabling change tracking and rollback.

\textbf{Repeatability:} The same template can be used to recreate identical infrastructure in different regions or accounts, ensuring consistency.

\textbf{Automation:} Scaling policies and health-based recovery are fully automated, removing manual intervention from the critical path to availability.

These principles ensure that the infrastructure can be reliably recreated, modified, and maintained throughout the project lifecycle and beyond.
