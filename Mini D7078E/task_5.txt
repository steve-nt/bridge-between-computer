================================================================================
TASK 5: SAFE FAILURE SIMULATION & RECOVERY
================================================================================

OBJECTIVE:
Simulate a controlled failure on ONE instance and observe:
âœ“ Health check detection of failure
âœ“ ALB removing unhealthy instance from traffic
âœ“ ASG launching replacement instance
âœ“ Automatic recovery and failover
âœ“ System remaining available during failure

REGION: eu-north-1
ASG: D7078E-MINI-PROJECT-GROUP35-ASG
ALB DNS: D7078E-MINI-PROJECT-GROUP35-LB-8834940.eu-north-1.elb.amazonaws.com

STATUS: âœ… READY (requires 3 instances)

================================================================================
PREREQUISITES FOR TASK 5
================================================================================

Before starting Task 5, verify:

âœ“ Task 4 completed (load scaling demonstrated)
âœ“ ASG has scaled to 3 instances
âœ“ All 3 instances are healthy in Target Group
âœ“ HealthyHostCount = 3 in CloudWatch

If NOT at 3 instances:
  â””â”€ Run more load (see SCALE_TO_3_INSTANCES.txt)
  â””â”€ Wait for scaling to complete
  â””â”€ Verify in EC2 console: Instances tab
  â””â”€ Verify in Target Group: All 3 showing "Healthy"

================================================================================
STEP 1: VERIFY 3 INSTANCES RUNNING
================================================================================

Before simulating failure:

1. Go to EC2 Console > Instances
   Check: All 3 instances from ASG show "Running" state

2. Go to EC2 Console > Target Groups > D7078E-MINI-PROJECT-GROUP35-TG
   Check: All 3 targets show "Healthy" status

3. Go to CloudWatch Dashboard
   Check: HealthyHostCount = 3

4. Test ALB connectivity
   curl http://D7078E-MINI-PROJECT-GROUP35-LB-8834940.eu-north-1.elb.amazonaws.com/
   Result: Should return 200 OK

If all checks pass, proceed to Step 2.

================================================================================
STEP 2: CHOOSE FAILURE SIMULATION METHOD
================================================================================

THREE OPTIONS (pick one):

OPTION A: AWS Fault Injection Simulator (FIS) - MOST PROFESSIONAL
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Best for: Production-like testing, auditable, reversible

Advantages:
  âœ“ Safe and designed for chaos testing
  âœ“ Fully auditable (CloudTrail logs)
  âœ“ Can set exact parameters (CPU, duration)
  âœ“ Easy to stop/rollback
  âœ“ Works at hypervisor level (no agent needed)

Steps:
  1. AWS Console > Fault Injection Simulator > Create experiment
  2. Name: "D7078E-Failure-Simulation"
  3. Service: EC2
  4. Action type: CPU load increase
  5. Target: Select ONE instance from your ASG
  6. Parameters:
     - CPU load: 90%
     - Duration: 120 seconds
  7. Click "Start experiment"
  8. Monitor CloudWatch during test

Time: 2-3 minutes
Cost: Free (part of AWS)
Complexity: Medium

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

OPTION B: SSM Stress Command - EASIEST & FASTEST (RECOMMENDED)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Best for: Quick testing, simple setup, no extra tools

Advantages:
  âœ“ Very simple command
  âœ“ Automatic timeout (safe)
  âœ“ Runs inside instance (no SSH needed)
  âœ“ Quick setup
  âœ“ No special IAM setup needed

Command to run:

aws ssm send-command \
  --instance-ids i-0a1b2c3d4e5f6g7h8 \
  --document-name "AWS-RunShellScript" \
  --parameters "commands=['sudo apt-get update -y','sudo apt-get install -y stress-ng','stress-ng -c 0 -l 90 -t 120s']" \
  --region eu-north-1

Replace: i-0a1b2c3d4e5f6g7h8 with actual instance ID

What happens:
  1. SSM agent on instance receives command
  2. Updates package manager
  3. Installs stress-ng tool
  4. Stress-ng runs for 120 seconds at 90% CPU
  5. Automatically stops after 120 seconds
  6. Instance recovers

Time: 2-3 minutes
Cost: Free
Complexity: Low

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

OPTION C: Direct SSH Stress - ADVANCED
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Best for: If you have SSH access and want full control

Command:
ssh -i D7078E-MINI-PROJECT-GROUP35-WEBSRV.pem ec2-user@<INSTANCE_IP>
stress-ng -c 0 -l 90 -t 120s

But Option B (SSM) is better - no SSH setup needed!

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

RECOMMENDED: Use Option B (SSM Stress Command)
  â””â”€ Simplest to execute
  â””â”€ No special setup
  â””â”€ Automatic timeout (safe)
  â””â”€ Good evidence for lab report

================================================================================
STEP 3: IDENTIFY INSTANCE TO STRESS
================================================================================

Choose ONE instance from your ASG to simulate failure on.

Go to: EC2 Console > Instances

Find instances from your ASG (look for tag or name matching "D7078E-MINI-PROJECT-GROUP35-ASG")

Copy the Instance ID of ONE instance:
  Example: i-0a1b2c3d4e5f6g7h8

Note down:
  â”œâ”€ Instance ID: i-____________
  â”œâ”€ Private IP: 172.31.__.__
  â”œâ”€ Launch time: ________
  â””â”€ Status: Running

================================================================================
STEP 4: START MONITORING (CRITICAL!)
================================================================================

Before running stress command, set up monitoring:

1. Open CloudWatch Dashboard in one browser tab
   â””â”€ URL: https://eu-north-1.console.aws.amazon.com/cloudwatch/
   â””â”€ Dashboard: D7078E-Mini-Project-Dashboard
   â””â”€ Set refresh: 10 seconds
   â””â”€ Watch: CPU, RequestCount, HealthyHostCount

2. Open Target Group in another browser tab
   â””â”€ EC2 > Target Groups > D7078E-MINI-PROJECT-GROUP35-TG
   â””â”€ Watch the 3 targets for health status changes
   â””â”€ Refresh every 30 seconds

3. Open ASG Activity in another browser tab
   â””â”€ EC2 > Auto Scaling Groups > D7078E-MINI-PROJECT-GROUP35-ASG
   â””â”€ Activity tab
   â””â”€ Watch for "Launching 1 instance" messages

4. Start a terminal/PowerShell for the stress command

All 4 windows open and ready to observe!

================================================================================
STEP 5: RUN STRESS COMMAND
================================================================================

Using Option B (SSM Stress Command - RECOMMENDED):

Copy this command and REPLACE the instance ID:

aws ssm send-command \
  --instance-ids i-YOUR_INSTANCE_ID \
  --document-name "AWS-RunShellScript" \
  --parameters "commands=['sudo apt-get update -y','sudo apt-get install -y stress-ng','stress-ng -c 0 -l 90 -t 120s']" \
  --region eu-north-1

Example with real instance ID:
aws ssm send-command \
  --instance-ids i-0a1b2c3d4e5f6g7h8 \
  --document-name "AWS-RunShellScript" \
  --parameters "commands=['sudo apt-get update -y','sudo apt-get install -y stress-ng','stress-ng -c 0 -l 90 -t 120s']" \
  --region eu-north-1

Execute the command.

Expected output:
  {
    "Command": {
      "CommandId": "abc-123-def",
      "Status": "Pending"
    }
  }

Note the CommandId for later reference.

================================================================================
STEP 6: OBSERVE THE FAILURE TIMELINE (2-3 MINUTES)
================================================================================

Minute 0 - Command executes
  â”œâ”€ Stress-ng starts on target instance
  â””â”€ CloudWatch: CPU on that instance spikes to 90%

Minute 0-0.5
  â”œâ”€ CloudWatch CPU graph shows spike
  â”œâ”€ Instance 1 CPU: 5% (normal)
  â”œâ”€ Instance 2 CPU: 5% (normal)
  â”œâ”€ Instance 3 (stressed) CPU: 90% SPIKE!
  â””â”€ Overall ASG CPU: Average of all 3 (â‰ˆ 33%)

Minute 0.5-1
  â”œâ”€ ALB health check (every 30 seconds) detects high CPU
  â”œâ”€ Health check FAILS (instance not responding in time)
  â”œâ”€ Target status changes: "Healthy" â†’ "Unhealthy"
  â”œâ”€ ALB removes instance from traffic
  â”œâ”€ RequestCount per instance drops (2 instances handling traffic now)
  â”œâ”€ HealthyHostCount: 3 â†’ 2 (unhealthy instance removed!)
  â””â”€ CloudWatch shows HealthyHostCount drop

Minute 1-1.5
  â”œâ”€ ASG detects unhealthy instance
  â”œâ”€ ASG launches REPLACEMENT instance
  â”œâ”€ ASG Activity shows: "Launching 1 instance"
  â”œâ”€ Replacement instance starts booting
  â”œâ”€ Old stressed instance still running (but not receiving traffic)
  â”œâ”€ HealthyHostCount still = 2 (replacement not healthy yet)
  â””â”€ ASG starting termination of unhealthy instance

Minute 1.5-2
  â”œâ”€ Stressed instance: Stress-ng nearing end (approaching 120s)
  â”œâ”€ Replacement instance: Booting (status changing to "Running")
  â”œâ”€ Replacement instance: Health checks starting
  â”œâ”€ HealthyHostCount still = 2
  â”œâ”€ RequestCount: Gradually rising (new instance coming online)
  â””â”€ Overall CPU: Rising again (stress still happening)

Minute 2
  â”œâ”€ Stress-ng STOPS (120 seconds complete)
  â”œâ”€ Stressed instance CPU: Drops from 90% â†’ 5%
  â”œâ”€ Instance still showing unhealthy (health check recovers slowly)
  â”œâ”€ ASG: Begins terminating unhealthy instance
  â”œâ”€ RequestCount: Still with 2 instances
  â””â”€ System recovering

Minute 2-2.5
  â”œâ”€ Replacement instance health checks: PASS
  â”œâ”€ Replacement instance status: "Healthy"
  â”œâ”€ HealthyHostCount: 2 â†’ 3 (recovery complete!)
  â”œâ”€ ALB sends traffic to replacement instance
  â”œâ”€ RequestCount rising again
  â”œâ”€ Old stressed instance: Terminates
  â””â”€ RECOVERY COMPLETE! âœ“

Minute 3+
  â”œâ”€ ASG: 3 instances running
  â”œâ”€ All instances healthy
  â”œâ”€ HealthyHostCount: 3
  â”œâ”€ CPU: Back to normal
  â”œâ”€ RequestCount: Back to normal
  â””â”€ System fully recovered!

TOTAL RECOVERY TIME: ~2-3 minutes

================================================================================
STEP 7: TAKE SCREENSHOTS DURING FAILURE
================================================================================

Screenshot at each critical moment:

â˜ Screenshot 1: Before failure (3 healthy hosts, normal metrics)
  â””â”€ CloudWatch: CPU, RequestCount, HealthyHostCount normal
  â””â”€ Target Group: All 3 showing "Healthy"

â˜ Screenshot 2: During stress (health check failing)
  â””â”€ CloudWatch: One instance CPU at 90%
  â””â”€ Target Group: One instance showing "Unhealthy" or "Draining"
  â””â”€ HealthyHostCount: Dropping 3 â†’ 2

â˜ Screenshot 3: Replacement launching
  â””â”€ EC2 Instances: New instance in "Pending" or "Running" status
  â””â”€ ASG Activity: "Launching 1 instance"
  â””â”€ HealthyHostCount: Still 2

â˜ Screenshot 4: Recovery (replacement healthy)
  â””â”€ CloudWatch: HealthyHostCount back to 3
  â””â”€ Target Group: All 3 "Healthy" again
  â””â”€ Metrics: Normalizing

â˜ Screenshot 5: After recovery (full system healthy)
  â””â”€ All metrics normal
  â””â”€ All 3 instances healthy
  â””â”€ No failed requests

These 5 screenshots tell the complete story of failure and recovery!

================================================================================
STEP 8: DOCUMENT OBSERVATIONS
================================================================================

Record these exact measurements:

Timeline:
  - Stress command executed: ____:____ (HH:MM)
  - Health check failed: ____:____ (HH:MM)
  - Instance marked unhealthy: ____:____ (HH:MM)
  - Replacement instance launched: ____:____ (HH:MM)
  - Stress ended (120s): ____:____ (HH:MM)
  - Replacement instance healthy: ____:____ (HH:MM)
  - System fully recovered: ____:____ (HH:MM)
  - Total recovery time: ____ minutes

Metrics:
  - Peak CPU on stressed instance: ____%
  - Average CPU on healthy instances: ____%
  - HealthyHostCount: Went from 3 â†’ 2 â†’ 3
  - RequestCount during failure: _____ RPS
  - RequestCount during recovery: _____ RPS
  - Failed requests during failure: _____
  - Successful requests during failure: _____

Observations:
  â–¡ Did health check correctly detect the failure?
  â–¡ How long did health check take to fail? ___ seconds
  â–¡ Did ALB remove unhealthy instance immediately?
  â–¡ Did ASG launch replacement immediately?
  â–¡ How long did replacement instance take to boot? ___ minutes
  â–¡ Were any requests lost during failure?
  â–¡ Did users see any errors (5xx responses)?
  â–¡ Did system remain available (some responses successful)?
  â–¡ How long until system was fully operational again? ___ minutes

Lessons Learned:
  - System automatically detected failure âœ“
  - System automatically replaced failed instance âœ“
  - No manual intervention needed âœ“
  - Downtime: Minimal (only 2-3 minutes)
  - Users continued to receive responses (from 2 healthy instances)
  - This is HIGH AVAILABILITY in action!

================================================================================
STEP 9: VERIFY SSM COMMAND EXECUTION (Optional)
================================================================================

To see command output:

aws ssm get-command-invocation \
  --command-id abc-123-def \
  --instance-id i-YOUR_INSTANCE_ID \
  --region eu-north-1

This shows:
  - Command execution status
  - Output from each step
  - Stress-ng results
  - Timing information

Great for lab report documentation!

================================================================================
STEP 10: CLEANUP AFTER TASK 5
================================================================================

After observing failure and recovery:

1. Stop load generators (if still running)
   â””â”€ Press Ctrl+C in load generator terminals

2. Wait for scale-in (optional)
   â””â”€ If you want to see scale-down, wait 10-15 minutes
   â””â”€ Instances will terminate when CPU stays below 30%

3. Proceed to Task 6 (Teardown) OR

4. Continue testing (run another failure simulation)

Task 5 is now COMPLETE! 

===============================================================================
DELIVERABLES FOR TASK 5
================================================================================

Collect for lab report:

â–¡ Screenshots: 5 critical moments (before, during, recovery, after)
â–¡ CloudWatch metrics: Full timeline showing failure and recovery
â–¡ ASG Activity log: Shows instance termination and replacement
â–¡ Target Group health status: Shows transition from 3 â†’ 2 â†’ 3
â–¡ EC2 Instances: Shows old instance terminating, new instance launching
â–¡ SSM Command output: Shows stress-ng execution details
â–¡ Timeline document: Exact timestamps of all events
â–¡ Analysis: What happened, how long recovery took
â–¡ Lessons learned: What this demonstrates about high availability

Analysis to include:
  âœ“ Automatic failure detection time: ___ seconds
  âœ“ Time to remove unhealthy instance: ___ seconds
  âœ“ Time to launch replacement: ___ seconds
  âœ“ Time for replacement to become healthy: ___ minutes
  âœ“ Total recovery time: ___ minutes
  âœ“ System availability during failure: Remained 66% available (2/3 healthy)
  âœ“ Manual intervention required: NONE (fully automatic!)

================================================================================
TASK 5 SUMMARY - âœ… COMPLETE
================================================================================

You have demonstrated:

âœ“ Health checks work correctly (detected failure in ~30 seconds)
âœ“ ALB removes failed instances (traffic redirected automatically)
âœ“ ASG replaces failed instances (new instance launched within 1 minute)
âœ“ System maintains availability (2/3 instances served traffic)
âœ“ Automatic recovery (no manual intervention)
âœ“ Zero-downtime failover (users had uninterrupted service)

This is PRODUCTION-GRADE reliability architecture! ğŸ¯

The system successfully recovered from a single instance failure:
  - Without any manual action
  - In less than 3 minutes
  - While maintaining service to users
  - With automatic replacement

This is what high availability and auto-scaling enable! ğŸš€

================================================================================
NEXT TASK: Task 6 - Teardown & Cleanup
================================================================================

After Task 5, complete Task 6:

1. Collect all evidence (screenshots, logs, metrics)
2. Stop any remaining load generators
3. Delete all AWS resources (ASG, ALB, Target Group, etc.)
4. Verify everything is cleaned up
5. Create final lab report

Task 5 is a demonstration of resilience.
Task 6 is cleanup and documentation.

Complete the lab successfully! âœ…

================================================================================
