================================================================================
TASK 4: IMPLEMENT & RUN INTERNAL LOAD GENERATORS
================================================================================

OBJECTIVE:
Deploy Python load generators in Docker containers to generate HTTP traffic
to the ALB. Monitor scaling as CPU rises and triggers auto-scaling policies.

REGION: eu-north-1
ALB DNS: D7078E-MINI-PROJECT-GROUP35-LB-8834940.eu-north-1.elb.amazonaws.com
ASG: D7078E-MINI-PROJECT-GROUP35-ASG
Max instances: 3

STATUS: Starting Task 4
Date: 2025-12-27 18:08:53 UTC

================================================================================
PREREQUISITES
================================================================================

Before starting, ensure you have:
✓ Python 3 installed on your local machine
✓ Docker installed and running
✓ Docker Compose installed
✓ ALB is responding to requests (from Task 2)
✓ Scaling policies and alarms are active (from Task 3)
✓ 1 instance currently running in the ASG

Quick verification:
- Test ALB: curl http://D7078E-MINI-PROJECT-GROUP35-LB-8834940.eu-north-1.elb.amazonaws.com/
- Check ASG: aws ec2 describe-instances --region eu-north-1 (should see 1 running)
- Check alarms: CloudWatch > Alarms (should see both alarms)

================================================================================
STEP 1: GET THE PYTHON LOAD GENERATOR SCRIPT (agent.py)
================================================================================

The lab already includes agent.py. Location:
C:\Users\snten\Desktop\bridge-between-computer\Mini D7078E\agent.py

If you don't have it, here's a simple implementation:

--- Create agent.py ---

cat > agent.py << 'EOF'
import asyncio
import aiohttp
import time
import argparse
from datetime import datetime

class LoadGenerator:
    def __init__(self, target_url, workers=2, rps=1, duration=300):
        self.target_url = target_url
        self.workers = workers
        self.rps = rps  # requests per second per worker
        self.duration = duration
        self.requests_sent = 0
        self.requests_success = 0
        self.requests_failed = 0
        self.start_time = None
        self.endpoints = ['/', '/metrics', '/burn']
    
    async def make_request(self, session, endpoint):
        try:
            url = self.target_url + endpoint
            async with session.get(url, timeout=10) as response:
                if response.status == 200:
                    self.requests_success += 1
                else:
                    self.requests_failed += 1
                self.requests_sent += 1
                return True
        except Exception as e:
            self.requests_failed += 1
            self.requests_sent += 1
            return False
    
    async def worker(self, session):
        """Each worker sends requests at configured RPS"""
        delay = 1.0 / self.rps  # delay between requests
        while time.time() - self.start_time < self.duration:
            endpoint = self.endpoints[self.requests_sent % len(self.endpoints)]
            await self.make_request(session, endpoint)
            await asyncio.sleep(delay)
    
    async def run(self):
        """Run the load generator with multiple workers"""
        self.start_time = time.time()
        connector = aiohttp.TCPConnector(limit=100)
        
        async with aiohttp.ClientSession(connector=connector) as session:
            tasks = [self.worker(session) for _ in range(self.workers)]
            await asyncio.gather(*tasks)
        
        elapsed = time.time() - self.start_time
        print(f"\n[{datetime.now()}] Load generation complete!")
        print(f"Duration: {elapsed:.1f} seconds")
        print(f"Total requests sent: {self.requests_sent}")
        print(f"Successful: {self.requests_success}")
        print(f"Failed: {self.requests_failed}")
        print(f"Actual RPS: {self.requests_sent / elapsed:.2f}")

async def main():
    parser = argparse.ArgumentParser(description='D7078E Load Generator')
    parser.add_argument('--url', required=True, help='Target URL (e.g., http://alb-dns/)')
    parser.add_argument('--workers', type=int, default=2, help='Number of concurrent workers')
    parser.add_argument('--rps', type=float, default=1, help='Requests per second per worker')
    parser.add_argument('--duration', type=int, default=300, help='Duration in seconds')
    
    args = parser.parse_args()
    
    print(f"[{datetime.now()}] Starting load generator")
    print(f"Target: {args.url}")
    print(f"Workers: {args.workers}")
    print(f"RPS per worker: {args.rps}")
    print(f"Total RPS: {args.workers * args.rps}")
    print(f"Duration: {args.duration} seconds")
    
    generator = LoadGenerator(
        target_url=args.url,
        workers=args.workers,
        rps=args.rps,
        duration=args.duration
    )
    await generator.run()

if __name__ == '__main__':
    asyncio.run(main())
EOF

Save as: agent.py

================================================================================
STEP 2: CREATE DOCKERFILE
================================================================================

cat > Dockerfile << 'EOF'
FROM python:3.11-slim

WORKDIR /app

COPY agent.py .

RUN pip install aiohttp

ENTRYPOINT ["python", "agent.py"]
EOF

Save as: Dockerfile

================================================================================
STEP 3: CREATE DOCKER-COMPOSE.YML
================================================================================

cat > docker-compose.yml << 'EOF'
version: '3.8'

services:
  agent1:
    build: .
    container_name: load-generator-agent1
    environment:
      - TARGET_URL=http://D7078E-MINI-PROJECT-GROUP35-LB-8834940.eu-north-1.elb.amazonaws.com/
    command: >
      --url http://D7078E-MINI-PROJECT-GROUP35-LB-8834940.eu-north-1.elb.amazonaws.com/
      --workers 2
      --rps 1
      --duration 600
    networks:
      - load-gen-network

networks:
  load-gen-network:
    driver: bridge
EOF

Save as: docker-compose.yml

================================================================================
STEP 4: BUILD DOCKER IMAGE
================================================================================

Command:
docker build -t d7078e-load-gen .

Expected output:
[+] Building 10.2s (7/7) FINISHED
 => [internal] load build context
 => [1/7] FROM python:3.11-slim
 => [2/7] WORKDIR /app
 => [3/7] COPY agent.py .
 => [4/7] RUN pip install aiohttp
 => exporting to image
 => => naming to docker.localized:latest

Result: ✓ Image built successfully

================================================================================
STEP 5: RUN LOAD GENERATOR - PHASE 1 (LOW LOAD)
================================================================================

Objective: Start with LOW load to establish baseline, then ramp up.

Command:
docker-compose up --scale agent1=1

This runs:
- 1 container
- 2 workers per container
- 1 RPS per worker
- Total: 2 RPS (low load)
- Duration: 600 seconds (10 minutes)

Expected behavior:
- Container starts and begins sending requests
- CPU should be LOW (< 30%)
- No scaling should occur
- Alarms remain in OK state
- Watch terminal for "requests_sent" counter increasing

Monitor in CloudWatch:
- Open CloudWatch dashboard
- Watch CPU stay below 30%
- Watch RequestCount increase

Duration: Run for 2-3 minutes, then proceed to Phase 2

Stop command: Ctrl+C (in terminal)

================================================================================
STEP 6: RUN LOAD GENERATOR - PHASE 2 (MEDIUM LOAD)
================================================================================

After stopping Phase 1, increase load:

Command:
docker-compose up --scale agent1=3

This runs:
- 3 containers (3 agents in parallel)
- 2 workers per container = 6 total workers
- 1 RPS per worker = 6 total RPS
- Total: 6 RPS (medium load)
- Duration: 600 seconds (10 minutes)

Expected behavior:
- CPU should rise toward 50-70%
- Requests per second should increase
- Still below 80% threshold, so no scaling yet
- Alarms remain in OK state

Monitor in CloudWatch:
- Watch CPU rise to 50-70%
- Watch RequestCount increase significantly
- Watch for smooth request handling

Duration: Run for 3-5 minutes, then proceed to Phase 3

Stop command: Ctrl+C

================================================================================
STEP 7: RUN LOAD GENERATOR - PHASE 3 (HIGH LOAD - TRIGGER SCALING)
================================================================================

Now increase load to trigger scaling:

Command:
docker-compose up --scale agent1=5

This runs:
- 5 containers (5 agents in parallel)
- 2 workers per container = 10 total workers
- 1 RPS per worker = 10 total RPS
- Total: 10 RPS (high load)
- Duration: 600 seconds (10 minutes)

Expected behavior:
TIMELINE:
- 0-2 minutes: CPU rises toward 80%
- 2+ minutes: When CPU >= 80% for 2 consecutive minutes
  → cpu-high-alarm-80percent triggers
  → scale-out-policy executes
  → +1 instance added (ASG: 1 → 2)
  → New instance boots (takes 1-2 minutes)
  
- 4+ minutes: Second instance now running
  → Load distributed across 2 instances
  → CPU may drop slightly, then rise again as more requests come in
  
- 5+ minutes: If load sustained and CPU still >= 80%
  → Another scale-out triggered
  → +1 instance added (ASG: 2 → 3)
  → Third instance boots

- 6+ minutes: All 3 instances running
  → Load fully distributed
  → ASG at maximum capacity (3)
  → CPU may stabilize or continue high depending on load

Monitor in CloudWatch Dashboard:
- Watch CPU: starts rising toward 80%
- Watch RequestCount: increases to 10+ RPS
- Watch HealthyHostCount: goes from 1 → 2 → 3
- Watch Alarms: cpu-high-alarm-80percent transitions OK → IN_ALARM
- Watch ASG Activity: new instances launching

Check EC2 Instances:
- Go to EC2 > Instances
- Watch instance count increase from 1 → 2 → 3
- Note timestamps of each scaling event

IMPORTANT OBSERVATIONS TO RECORD:
1. Time when cpu-high-alarm-80percent triggered
2. Time when first instance was added (ASG: 1 → 2)
3. Time when second instance was added (ASG: 2 → 3)
4. CPU values at each point
5. Request count before and after scaling
6. How load was distributed across instances

Duration: Run for 10-15 minutes to observe full scaling

================================================================================
STEP 8: MONITOR SCALING EVENTS
================================================================================

While load is running, monitor these locations:

1. CloudWatch Dashboard:
   - Go to CloudWatch > Dashboards > D7078E-Lab-Dashboard
   - Watch CPU, RequestCount, HealthyHostCount in real-time
   - Take screenshots of each major event

2. ASG Activity:
   - Go to EC2 > Auto Scaling Groups > D7078E-MINI-PROJECT-GROUP35-ASG
   - Click "Activity" tab
   - Watch scaling events appear in real-time
   - Screenshot each scaling event

3. CloudWatch Alarms:
   - Go to CloudWatch > Alarms
   - Click on cpu-high-alarm-80percent
   - Watch state transition: OK → IN_ALARM
   - Screenshot when it triggers

4. EC2 Instances:
   - Go to EC2 > Instances
   - Filter by ASG
   - Watch instance count increase
   - Note instance IDs and launch times

5. Terminal Output:
   - Watch agent.py output showing requests being sent
   - Watch for any errors or dropped requests

================================================================================
STEP 9: OBSERVE SCALE-IN (WHEN LOAD DECREASES)
================================================================================

After Phase 3 completes or when you're ready to scale down:

Option A: Stop the load generator
Command: Ctrl+C (in terminal running docker-compose)

Option B: Scale down to fewer containers
Command: docker-compose up --scale agent1=1

Expected behavior:
- Load decreases
- CPU drops below 80%
- cpu-high-alarm-80percent transitions IN_ALARM → OK
- Wait 5+ minutes for cpu-low-alarm-30percent to trigger
- When CPU <= 30% for 5 consecutive minutes:
  → cpu-low-alarm-30percent triggers
  → scale-in-policy executes
  → -1 instance removed (ASG: 3 → 2)
  → Instance terminates gracefully

Monitor:
- Watch CPU drop over time
- Watch RequestCount decrease
- Watch HealthyHostCount decrease (3 → 2 → 1)
- Note timing of scale-in events

IMPORTANT: The scale-in may take 10+ minutes total because:
- Takes 5 minutes of low CPU to trigger the alarm
- Instance may take 1-2 minutes to terminate
- Total: 6-7 minutes from when load stops

================================================================================
STEP 10: CLEANUP AND DATA COLLECTION
================================================================================

After observing scaling:

1. Stop the load generator:
   Ctrl+C (in the docker-compose terminal)

2. Stop and remove containers:
   docker-compose down

3. Collect data:
   - Screenshot CloudWatch metrics
   - Export ASG activity log
   - Export CloudWatch alarms history
   - Save terminal output from load generator

4. Document observations:
   - When did scaling events occur?
   - How long did each instance take to boot?
   - What was the peak CPU?
   - What was peak RequestCount?
   - How many instances reached maximum (3)?
   - Did any instances fail health checks?

5. Calculate metrics:
   - Time from alarm trigger to instance available
   - Total requests served during test
   - Average RPS achieved
   - Peak RPS
   - Success rate

================================================================================
DELIVERABLES FOR TASK 4:
================================================================================

□ Screenshot: CloudWatch dashboard during low load (Phase 1)
□ Screenshot: CloudWatch dashboard during medium load (Phase 2)
□ Screenshot: CloudWatch dashboard during high load (Phase 3)
□ Screenshot: CloudWatch alarms transitioning to IN_ALARM
□ Screenshot: EC2 instances showing 1 → 2 → 3 instances
□ Screenshot: ASG Activity tab showing scaling events
□ Screenshot: Final CPU and RequestCount metrics
□ Document: Exact timestamps of each scaling event
□ Document: Peak CPU values at each phase
□ Document: RequestCount before/after each scaling
□ Terminal output: agent.py final statistics
□ Timeline: Event timeline from start to completion
□ Observations: What worked, what failed, lessons learned

================================================================================
TASK 4 SUMMARY
================================================================================

You will have demonstrated:
✓ Load generation with controlled traffic
✓ Auto-scaling triggered by CloudWatch alarms
✓ Scaling-out: 1 instance → 2 instances → 3 instances
✓ Load distribution across multiple instances
✓ CloudWatch alarm and metric monitoring
✓ Real-time observation of infrastructure scaling

Results should show:
✓ CPU rising from 0% → 80%+
✓ Alarms triggering at correct thresholds
✓ New instances launching on demand
✓ Load balanced across all instances
✓ No failed requests or health check failures

================================================================================
NEXT TASK: Task 5 - Safe Failure Simulation
================================================================================

After completing Task 4, you will:
1. Trigger a controlled failure (FIS or SSM stress command)
2. Observe health checks and instance replacement
3. Watch recovery and failover behavior
4. Document all events

This completes the scaling and failure observation lab!

================================================================================
