Lab 1 Report D7079E

Group 12
Fenix Kanat fenix.kanat97@gmail.com
Joel Tulehag joetul-4@student.ltu.se
Stylianos Zouridakis styzou-5@student.ltu.se
Stefanos Ntentopoulos 






































































Table of Contents


Table of Contents        1
Part I        2
Task 1.1        2
Task 1.2        4
Task 1.3        4
Task 1.4        4
Task 1.5        5
Task 2        5
Part II        5
Task 3.1        5
Task 3.2        5
Task 3 questions        5
Task 4        6
Task 4.2        6
Task 4 questions        6




























































Part I
Task 1.1


We started by following the preprocessing steps from the guide. 

* First step was mean normalization to center the data, resulting on this result:
  
The picture is still visible as the differences between the values are still the same but we can see a small smoothening on the colors
* Next was standardization of these centered values, by dividing each dimension by its standard deviation. Our image is still visible. 
  
The black color on the edges are “dead pixels” having value 0. By dividing with their standard deviation we perform x/0 calculation resulting on these dead pixels and an error on code.
* The last step was whitening of the dataset. To achieve this we need to do decorrelation of the centered data and then divide these values with the square root of their eigenvalue.
This is after decorrelation:
   
We can clearly see that the image is not visible anymore, this is expected as by this action, we rotate the coordinate system so that each pixel is completely independent from the others.
And here it’s after whitening of the decorrelated data:
  
This is also expected as now the pixel values are normalized scaling down strong features and scaling up weak features.






According to the guide for an image dataset (like the MNIST in our case) we need to use ZCA whitening in order to have a visible image. To use this method we first reshaped the dataset from (10000,28,28) to (10000,784) to have flat image data. Then we rescaled the pixel values on the range [0,1] by dividing by 255 (the maximum value of a pixel). 
The final result was this:
  


Task 1.2
After running the provided code we achieved a classification accuracy of 0.2649, which is relatively low. 


Task 1.3
After using the L2 norm to calculate the distances we achieved even lower classification accuracy at 0.19! 


Task 1.4
In order to achieve higher classification accuracy we need to rescale the data like we did in the preprocessing steps in Task 1.1. Since we are working with images in order to rescale the data on range [0,1] we need to divide by the maximum pixel value 255. After providing the rescaled dataset as an input to our functions we see these results:
  


Task 1.5
For this task we extended our function to implement k-NN classifiers. We used this function for k=3 and k=5 and we can see that the accuracy is decreasing:
  
k-NN is designed to be more robust against noise and it’s not necessarily increasing the accuracy. It appears that for our MNIST dataset the single closest neighbor is a good match and gives us the best accuracy, while when we include more neighbors we may include “distractor” images that may belong to different classes, causing the voting to fail. 
Task 2


Part II
Task 3.1
We implemented the pixel attack using Differential Evolution (DE). Each candidate solution represents one modified pixel as (x, y, r, g, b).


Mutation strategy: DE style mutation (difference of two individuals added to a base vector) and selection keeps the candidate that improves the objective.


Fitness function: Minimize the model’s confidence (softmax probability) for the true class. If the predicted class changes, the attack is counted as successful.


Constraint handling: Pixel coordinates are clamped to the image bounds (0–31) and RGB values are clamped to valid ranges (0–255).




Task 3.2
  Task 3 questions
1. Success Rate Analysis: What percentage of images can be successfully attacked?
The one-pixel attack succeeded on 86 out of 200 images, which is 43%. 


2. Efficiency Analysis: How many iterations does the algorithm typically need?
For successful attacks, the algorithm needed about 6.6 generations on average.




3. Pattern Recognition: Are certain types of images more vulnerable?
Yes. In this run, “cat” (81.8%) and “bird” (65.2%) were among the most vulnerable classes, while “truck” (11.5%) and “horse” (22.2%) were among the least vulnerable.


Task 4
We implemented an APR style defense based on three components:


Adversarial training: during training, batches include one pixel adversarial examples generated with the same attack method.


Pixel wise attention layer: We added a lightweight attention/gating layer to make the model less sensitive to single pixel changes.


Regularisation: We added a gradient based regularisation term to discourage extreme sensitivity to small input changes.




Task 4.2
Task 4 questions
1. Can you reproduce the effectiveness shown in the paper?
We reproduce the trend reported in the APR paper: APR reduces one pixel attack success rate compared to an undefended model. In our baseline run (N=200), ASR decreased from 43% to 31 (−12 points, −27.9% relative). The APR paper reports a larger reduction on CIFAR-10 (ASR 70.97% → 21.43%, ~69.8% relative) using substantially heavier training (200 epochs) and a larger DE population (400)


2. What is the trade-off between security and model accuracy?
Overall, APR increases robustness but can slightly reduce clean accuracy. In our 200 image run accuracy fell 76.69% → 72.11% while ASR fell 43% → 31%. .


3. Could an attacker adapt their strategy to overcome this defense?
Yes. An attacker could change their strategy to make the attack stronger and increase the chance of fooling the defended model. For example, they could let the attack run longer and try more different pixel changes, they could change more than one pixel instead of only one, or they could aim for one specific wrong label such as forcing a “cat” to be classified as a “dog”. These changes give the attacker more freedom, so the defense may be easier to bypass.